{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["VC6dgzIvNpYm","JLWC-e33ywAM","0E9B-fNYOB3q","rQFEcuNXpE8C"],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tox21 GINENet\n\nWe’re solving a multi-task binary toxicity prediction problem on Tox21. Concretely, for each molecule the model outputs 12 probabilities, one for each assay (e.g. NR-AR, SR-ARE, p53, etc.). At training time we use a binary cross-entropy loss (with masking for missing labels) over those 12 tasks, and at the end of each epoch we compute the ROC-AUC per task (then average) on the held-out validation set to see how well the model is distinguishing actives vs. inactives across all assays.","metadata":{"id":"VC6dgzIvNpYm"}},{"cell_type":"code","source":"%pip -q install rdkit-pypi torch_geometric","metadata":{"id":"l0Z81NMxoiW9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e17f73e4-4830-4b56-b3bc-34550217458f","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:43:40.039411Z","iopub.execute_input":"2025-05-25T15:43:40.039708Z","iopub.status.idle":"2025-05-25T15:43:46.765647Z","shell.execute_reply.started":"2025-05-25T15:43:40.039685Z","shell.execute_reply":"2025-05-25T15:43:46.764771Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Cell below must be run twice for some obscure reason","metadata":{"id":"S3s90ZFaDP3_"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Subset, WeightedRandomSampler\nfrom torch.serialization import safe_globals, add_safe_globals\n\n# Pytorch Geometric Imports\nfrom torch_geometric.data import InMemoryDataset, Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.data.data import DataTensorAttr, DataEdgeAttr\nfrom torch_geometric.data.storage import GlobalStorage\nfrom torch_geometric.nn import GINEConv, GatedGraphConv, global_mean_pool\n\n# RDKit Imports\nfrom rdkit.Chem.Scaffolds import MurckoScaffold\nfrom rdkit.Chem import MolFromSmiles, MolToSmiles, rdchem\n\nfrom sklearn.metrics import roc_auc_score\nimport os","metadata":{"id":"TmEESHMxt72N","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:43:46.767229Z","iopub.execute_input":"2025-05-25T15:43:46.767480Z","iopub.status.idle":"2025-05-25T15:43:56.565346Z","shell.execute_reply.started":"2025-05-25T15:43:46.767450Z","shell.execute_reply":"2025-05-25T15:43:56.564576Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!wget wget https://raw.githubusercontent.com/deepchem/deepchem/master/datasets/tox21.csv.gz\n!gunzip tox21.csv.gz\n!mkdir -p raw\n!mv tox21.csv raw/","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4dsjviCYyJpu","outputId":"1df76760-1456-4190-c35e-e668485b0878","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:43:56.566256Z","iopub.execute_input":"2025-05-25T15:43:56.566785Z","iopub.status.idle":"2025-05-25T15:43:57.365905Z","shell.execute_reply.started":"2025-05-25T15:43:56.566761Z","shell.execute_reply":"2025-05-25T15:43:57.364699Z"}},"outputs":[{"name":"stdout","text":"--2025-05-25 15:43:56--  http://wget/\nResolving wget (wget)... failed: Name or service not known.\nwget: unable to resolve host address ‘wget’\n--2025-05-25 15:43:56--  https://raw.githubusercontent.com/deepchem/deepchem/master/datasets/tox21.csv.gz\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 125310 (122K) [application/octet-stream]\nSaving to: ‘tox21.csv.gz’\n\ntox21.csv.gz        100%[===================>] 122.37K  --.-KB/s    in 0.02s   \n\n2025-05-25 15:43:56 (5.58 MB/s) - ‘tox21.csv.gz’ saved [125310/125310]\n\nFINISHED --2025-05-25 15:43:56--\nTotal wall clock time: 0.2s\nDownloaded: 1 files, 122K in 0.02s (5.58 MB/s)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"add_safe_globals([DataTensorAttr, DataEdgeAttr, GlobalStorage])","metadata":{"id":"D1iG2mLOokTP","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:43:57.367818Z","iopub.execute_input":"2025-05-25T15:43:57.368076Z","iopub.status.idle":"2025-05-25T15:43:57.372797Z","shell.execute_reply.started":"2025-05-25T15:43:57.368051Z","shell.execute_reply":"2025-05-25T15:43:57.372058Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Dataset","metadata":{"id":"JLWC-e33ywAM"}},{"cell_type":"markdown","source":"Perhaps we will add more features later/do some feature optimization","metadata":{}},{"cell_type":"code","source":"class Tox21Dataset(InMemoryDataset):\n    def __init__(self, root: str = '.', transform=None, pre_transform=None):\n        \"\"\"\n        Expects:\n        root/\n            raw/tox21.csv\n        Will create:\n            processed/data.pt\n        \"\"\"\n        super().__init__(root, transform, pre_transform)\n        # Load the processed data\n        with safe_globals([DataTensorAttr, DataEdgeAttr, GlobalStorage]):\n            self.data, self.slices = torch.load(self.processed_paths[0])\n\n    @property\n    def raw_file_names(self):\n        # File expected in root/raw/\n        return ['tox21.csv']\n\n    @property\n    def processed_file_names(self):\n        return ['data.pt']\n\n    def download(self):\n        # No download step needed; CSV is already in place\n        return\n\n    def process(self):\n        df = pd.read_csv(self.raw_paths[0])\n        df = df.fillna(value=0)\n        \n        if \"mol_id\" in df.columns:\n            df = df.drop(columns=[\"mol_id\"])\n\n        # 3) convert all non-smiles columns to numeric\n        non_smiles = [c for c in df.columns if c != \"smiles\"]\n        df[non_smiles] = df[non_smiles].apply(pd.to_numeric, errors=\"coerce\")\n        df = df.reset_index(drop=True)\n\n        data_list = []\n        bond_types = [\n            rdchem.BondType.SINGLE,\n            rdchem.BondType.DOUBLE,\n            rdchem.BondType.TRIPLE,\n            rdchem.BondType.AROMATIC,\n        ]\n\n        for _, row in df.iterrows():\n            smiles = row[\"smiles\"]\n            mol    = MolFromSmiles(smiles, sanitize=True)\n            if mol is None:\n                continue\n\n            # node features x\n            atom_feats = [\n                [\n                    a.GetAtomicNum(),\n                    a.GetDegree(),\n                    a.GetFormalCharge(),\n                    a.GetNumRadicalElectrons(),\n                ]\n                for a in mol.GetAtoms()\n            ]\n            x = torch.tensor(atom_feats, dtype=torch.float32)\n\n            # build edge_index AND edge_attr in lock‐step\n            edge_index = []\n            edge_attr  = []\n            for bond in mol.GetBonds():\n                i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n                # one‐hot encode this bond’s type\n                bt = bond.GetBondType()\n                feat = [int(bt == t) for t in bond_types]\n\n                # add both directions\n                edge_index += [[i, j], [j, i]]\n                edge_attr  += [feat, feat]\n\n            edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n            edge_attr  = torch.tensor(edge_attr, dtype=torch.float32)  # [2E, len(bond_types)]\n\n            # your labels\n            y = torch.tensor(row[[c for c in df.columns if c!=\"smiles\"]]\n                             .tolist(), dtype=torch.float32)\n\n            data_list.append(Data(\n                x=x,\n                edge_index=edge_index,\n                edge_attr=edge_attr,\n                y=y,\n            ))\n\n        data, slices = self.collate(data_list)\n        torch.save((data, slices), self.processed_paths[0])\n","metadata":{"id":"hVOQwpZ4yd21","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:44:14.796195Z","iopub.execute_input":"2025-05-25T15:44:14.796514Z","iopub.status.idle":"2025-05-25T15:44:14.810249Z","shell.execute_reply.started":"2025-05-25T15:44:14.796490Z","shell.execute_reply":"2025-05-25T15:44:14.809589Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"dataset = Tox21Dataset('.')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kTlchDlGFb1w","outputId":"ee3391fe-ad31-45d1-a11d-37ee5300bea3","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:44:17.458055Z","iopub.execute_input":"2025-05-25T15:44:17.458762Z","iopub.status.idle":"2025-05-25T15:44:25.608854Z","shell.execute_reply.started":"2025-05-25T15:44:17.458739Z","shell.execute_reply":"2025-05-25T15:44:25.607983Z"}},"outputs":[{"name":"stderr","text":"Processing...\n[15:44:17] WARNING: not removing hydrogen atom without neighbors\nDone!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"A **Murcko Scaffold** is a technique used to group molecules based on their core structural components. It identifies the essential building blocks by removing side chains and other non-core components, leaving behind the important ring systems and connecting chains. This method is widely used in medicinal chemistry and drug design to identify core structures that have preferential activity against specific targets, which is very useful in **molecular property prediction**.","metadata":{"id":"KjA7GOcFVrTE"}},{"cell_type":"code","source":"def GetMurckoScaffold(data, train_frac: float, val_frac: float, test_frac: float):\n    scaffold_indices = {}\n    for idx, smiles in enumerate(data):\n        scaffold_smiles = MurckoScaffold.MurckoScaffoldSmiles(smiles)\n        #scaffold_smiles = MolToSmiles(scaffold)\n        scaffold_indices.setdefault(scaffold_smiles, []).append(idx)\n\n    groups = sorted(scaffold_indices.values(), key=len, reverse=True)\n    n_total = len(data)\n    n_train = int(train_frac * n_total)\n    n_valid = int(val_frac * n_total)\n\n    train_idx, valid_idx, test_idx = [], [], []\n    for group in groups:\n        if len(train_idx) + len(group) <= n_train:\n            train_idx.extend(group)\n        elif len(valid_idx) + len(group) <= n_valid:\n            valid_idx.extend(group)\n        else:\n            test_idx.extend(group)\n\n    return train_idx, valid_idx, test_idx","metadata":{"id":"enkTOWoRzMiq","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:44:29.565145Z","iopub.execute_input":"2025-05-25T15:44:29.565893Z","iopub.status.idle":"2025-05-25T15:44:29.571346Z","shell.execute_reply.started":"2025-05-25T15:44:29.565866Z","shell.execute_reply":"2025-05-25T15:44:29.570492Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## GNN Model","metadata":{"id":"0E9B-fNYOB3q"}},{"cell_type":"code","source":"np.random.seed(1638)\ntorch.manual_seed(1638)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:44:31.261064Z","iopub.execute_input":"2025-05-25T15:44:31.261784Z","iopub.status.idle":"2025-05-25T15:44:31.272716Z","shell.execute_reply.started":"2025-05-25T15:44:31.261755Z","shell.execute_reply":"2025-05-25T15:44:31.271915Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7bfeb9e4f1f0>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"class GINEModel(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        num_tasks: int,\n        dropout: float,\n        n_layers: int\n    ):\n        super().__init__()\n        \n        self.conv1 = GINEConv(\n            nn.Sequential(\n                nn.Linear(input_dim, hidden_dim),\n                nn.BatchNorm1d(hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, hidden_dim),\n            ),\n            edge_dim = 4,\n        )\n\n        self.conv_block = nn.ModuleList()   \n        for _ in range(n_layers):\n            mlp = nn.Sequential(\n                nn.Linear(hidden_dim, hidden_dim),\n                nn.BatchNorm1d(hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, hidden_dim),\n            )\n            self.conv_block.append(GINEConv(mlp, edge_dim=4))\n\n        self.gate = GatedGraphConv(hidden_dim, 3) # 2nd arg is num of layers\n        self.bn_gate = nn.BatchNorm1d(hidden_dim)\n        self.pool = global_mean_pool\n        self.fc = nn.Linear(hidden_dim, num_tasks)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, edge_index, edge_attr, batch):\n        # 1st GINE layer\n        x = self.conv1(x, edge_index, edge_attr)\n        x = nn.Dropout(0.1)(x)\n\n        # GINE layers + skip connections\n        for block in self.conv_block:\n            h = block(x, edge_index, edge_attr)\n            x = (x + h).relu()\n            x = nn.Dropout(0.1)(x)\n    \n        # normalization, dropout, pooling, and final FC\n        x = self.bn_gate(x)\n        x = self.dropout(x)\n        x = self.pool(x, batch)\n        return self.fc(x)","metadata":{"id":"gWhA6HDyMM5J","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:06:12.905016Z","iopub.execute_input":"2025-05-25T16:06:12.905569Z","iopub.status.idle":"2025-05-25T16:06:12.913395Z","shell.execute_reply.started":"2025-05-25T16:06:12.905540Z","shell.execute_reply":"2025-05-25T16:06:12.912584Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## Train the Model","metadata":{"id":"rQFEcuNXpE8C"}},{"cell_type":"code","source":"def train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        if hasattr(batch, 'edge_attr'):\n            logits = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n        else:\n            logits = model(batch.x, batch.edge_index, batch.batch)\n        mask = (batch.y >= 0).float()\n        bs, nt = logits.size()\n        batch_y = batch.y.view(bs, nt)\n        mask = mask.view(bs, nt)\n        loss = (criterion(logits, batch_y) * mask).sum() / mask.sum()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch.num_graphs\n    return total_loss / len(loader.dataset)","metadata":{"id":"QI3ge3gPRtHb","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:02:42.939315Z","iopub.execute_input":"2025-05-25T16:02:42.939952Z","iopub.status.idle":"2025-05-25T16:02:42.945660Z","shell.execute_reply.started":"2025-05-25T16:02:42.939925Z","shell.execute_reply":"2025-05-25T16:02:42.944972Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def evaluate(model, loader, device):\n    model.eval()\n    y, preds = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            if hasattr(batch, 'edge_attr'):\n                logits = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n            else:\n                logits = model(batch.x, batch.edge_index, batch.batch)\n            y.append(batch.y.cpu())\n            preds.append(torch.sigmoid(logits).cpu())\n    return torch.cat(preds, dim=0).numpy(), torch.cat(y, dim=0).numpy()","metadata":{"id":"Ldql5-B3o-LB","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:02:43.048634Z","iopub.execute_input":"2025-05-25T16:02:43.048833Z","iopub.status.idle":"2025-05-25T16:02:43.053655Z","shell.execute_reply.started":"2025-05-25T16:02:43.048818Z","shell.execute_reply":"2025-05-25T16:02:43.052988Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Main","metadata":{"id":"3U04GeF_qONW"}},{"cell_type":"code","source":"df = pd.read_csv('raw/tox21.csv').fillna(0).reset_index(drop=True)\nsmiles_list = df['smiles'].tolist()\ntrain_idx, valid_idx, test_idx = GetMurckoScaffold(smiles_list, 0.8, 0.1, 0.1)\n\ntrain_ds = Subset(dataset, train_idx)\nval_ds   = Subset(dataset, valid_idx)\ntest_ds  = Subset(dataset, test_idx)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)#, sampler=sampler)\nval_loader   = DataLoader(val_ds,   batch_size=32, shuffle=True)\ntest_loader  = DataLoader(test_ds,  batch_size=32)\n\nsample = dataset[0]\nin_channels = sample.x.size(1)\nnum_tasks   = sample.y.size(0)\n\nprint(\"There are\", num_tasks, \"tasks.\")","metadata":{"id":"Xawk2eEFs2Sm","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:02:45.353901Z","iopub.execute_input":"2025-05-25T16:02:45.354290Z","iopub.status.idle":"2025-05-25T16:02:47.662070Z","shell.execute_reply.started":"2025-05-25T16:02:45.354255Z","shell.execute_reply":"2025-05-25T16:02:47.661420Z"}},"outputs":[{"name":"stderr","text":"[16:02:45] WARNING: not removing hydrogen atom without neighbors\n","output_type":"stream"},{"name":"stdout","text":"There are 12 tasks.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:02:47.663101Z","iopub.execute_input":"2025-05-25T16:02:47.663405Z","iopub.status.idle":"2025-05-25T16:02:47.678258Z","shell.execute_reply.started":"2025-05-25T16:02:47.663387Z","shell.execute_reply":"2025-05-25T16:02:47.677696Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"      NR-AR  NR-AR-LBD  NR-AhR  NR-Aromatase  NR-ER  NR-ER-LBD  NR-PPAR-gamma  \\\n8009    0.0        0.0     0.0           0.0    0.0        0.0            0.0   \n8010    1.0        1.0     0.0           0.0    1.0        0.0            0.0   \n8011    1.0        1.0     0.0           0.0    1.0        1.0            0.0   \n8012    1.0        1.0     0.0           0.0    1.0        1.0            0.0   \n8013    0.0        0.0     0.0           0.0    0.0        0.0            0.0   \n\n      SR-ARE  SR-ATAD5  SR-HSE  SR-MMP  SR-p53   mol_id  \\\n8009     0.0       0.0     0.0     0.0     0.0  TOX2725   \n8010     0.0       0.0     0.0     0.0     0.0  TOX2370   \n8011     1.0       0.0     0.0     0.0     0.0  TOX2371   \n8012     0.0       0.0     0.0     1.0     1.0  TOX2377   \n8013     0.0       0.0     0.0     1.0     0.0  TOX2724   \n\n                                                 smiles  \n8009  CCOc1nc2cccc(C(=O)O)c2n1Cc1ccc(-c2ccccc2-c2nnn...  \n8010  CC(=O)[C@H]1CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(...  \n8011  C[C@]12CC[C@H]3[C@@H](CCC4=CC(=O)CC[C@@]43C)[C...  \n8012  C[C@]12CC[C@@H]3c4ccc(O)cc4CC[C@H]3[C@@H]1CC[C...  \n8013            COc1ccc2c(c1OC)CN1CCc3cc4c(cc3C1C2)OCO4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NR-AR</th>\n      <th>NR-AR-LBD</th>\n      <th>NR-AhR</th>\n      <th>NR-Aromatase</th>\n      <th>NR-ER</th>\n      <th>NR-ER-LBD</th>\n      <th>NR-PPAR-gamma</th>\n      <th>SR-ARE</th>\n      <th>SR-ATAD5</th>\n      <th>SR-HSE</th>\n      <th>SR-MMP</th>\n      <th>SR-p53</th>\n      <th>mol_id</th>\n      <th>smiles</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8009</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>TOX2725</td>\n      <td>CCOc1nc2cccc(C(=O)O)c2n1Cc1ccc(-c2ccccc2-c2nnn...</td>\n    </tr>\n    <tr>\n      <th>8010</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>TOX2370</td>\n      <td>CC(=O)[C@H]1CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@]4(...</td>\n    </tr>\n    <tr>\n      <th>8011</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>TOX2371</td>\n      <td>C[C@]12CC[C@H]3[C@@H](CCC4=CC(=O)CC[C@@]43C)[C...</td>\n    </tr>\n    <tr>\n      <th>8012</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>TOX2377</td>\n      <td>C[C@]12CC[C@@H]3c4ccc(O)cc4CC[C@H]3[C@@H]1CC[C...</td>\n    </tr>\n    <tr>\n      <th>8013</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>TOX2724</td>\n      <td>COc1ccc2c(c1OC)CN1CCc3cc4c(cc3C1C2)OCO4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = GINEModel(\n    in_channels,\n    hidden_dim = 384,\n    num_tasks = num_tasks,\n    dropout = 0.2,\n    n_layers = 3\n).to(device)\n\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:06:16.765413Z","iopub.execute_input":"2025-05-25T16:06:16.766159Z","iopub.status.idle":"2025-05-25T16:06:16.817502Z","shell.execute_reply.started":"2025-05-25T16:06:16.766134Z","shell.execute_reply":"2025-05-25T16:06:16.816667Z"}},"outputs":[{"name":"stdout","text":"GINEModel(\n  (conv1): GINEConv(nn=Sequential(\n    (0): Linear(in_features=4, out_features=384, bias=True)\n    (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Linear(in_features=384, out_features=384, bias=True)\n  ))\n  (conv_block): ModuleList(\n    (0-2): 3 x GINEConv(nn=Sequential(\n      (0): Linear(in_features=384, out_features=384, bias=True)\n      (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): Linear(in_features=384, out_features=384, bias=True)\n    ))\n  )\n  (gate): GatedGraphConv(384, num_layers=3)\n  (bn_gate): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc): Linear(in_features=384, out_features=12, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"optimizer = optim.AdamW(\n    model.parameters(),\n    lr=1e-3,\n    weight_decay=1e-5,\n)\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode=\"max\",           # we track val AUC, so “max”\n    factor=0.5,           # halve the lr\n    patience=5,           # after 5 epochs with no AUC gain\n    min_lr=1e-6\n)\n\ncriterion = nn.BCEWithLogitsLoss(reduction='none')","metadata":{"id":"KjGvP1-jwVFM","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:06:21.766490Z","iopub.execute_input":"2025-05-25T16:06:21.766976Z","iopub.status.idle":"2025-05-25T16:06:21.772294Z","shell.execute_reply.started":"2025-05-25T16:06:21.766946Z","shell.execute_reply":"2025-05-25T16:06:21.771574Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"for epoch in range(1, 21):\n    loss = train_epoch(model, train_loader, optimizer, criterion, device)\n    if loss is None:\n        print(\"Error during training\")\n        break\n    ps_val, ys_val = evaluate(model, val_loader, device)\n    ys_val = ys_val.reshape(ps_val.shape)\n    aucs = []\n    for i in range(ps_val.shape[1]):\n        mask = ys_val[:, i] >= 0\n        if mask.sum() > 0:\n            aucs.append(roc_auc_score(ys_val[mask, i], ps_val[mask, i]))\n    scheduler.step(np.mean(aucs)) # we use the mean like below\n    print(f\"Epoch {epoch:02d} | Loss: {loss:.4f} | Val AUC: {np.mean(aucs):.3f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"id":"gp_XvcMsw8jW","outputId":"0af71082-b925-49a3-edc8-f8c8569144f3","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:06:22.879550Z","iopub.execute_input":"2025-05-25T16:06:22.879788Z","iopub.status.idle":"2025-05-25T16:07:03.675926Z","shell.execute_reply.started":"2025-05-25T16:06:22.879772Z","shell.execute_reply":"2025-05-25T16:07:03.675343Z"}},"outputs":[{"name":"stdout","text":"Epoch 01 | Loss: 0.3516 | Val AUC: 0.629\nEpoch 02 | Loss: 0.2002 | Val AUC: 0.651\nEpoch 03 | Loss: 0.1970 | Val AUC: 0.663\nEpoch 04 | Loss: 0.1947 | Val AUC: 0.647\nEpoch 05 | Loss: 0.1937 | Val AUC: 0.680\nEpoch 06 | Loss: 0.1908 | Val AUC: 0.669\nEpoch 07 | Loss: 0.1896 | Val AUC: 0.666\nEpoch 08 | Loss: 0.1871 | Val AUC: 0.670\nEpoch 09 | Loss: 0.1872 | Val AUC: 0.691\nEpoch 10 | Loss: 0.1854 | Val AUC: 0.698\nEpoch 11 | Loss: 0.1860 | Val AUC: 0.680\nEpoch 12 | Loss: 0.1838 | Val AUC: 0.699\nEpoch 13 | Loss: 0.1840 | Val AUC: 0.715\nEpoch 14 | Loss: 0.1816 | Val AUC: 0.703\nEpoch 15 | Loss: 0.1810 | Val AUC: 0.704\nEpoch 16 | Loss: 0.1804 | Val AUC: 0.702\nEpoch 17 | Loss: 0.1794 | Val AUC: 0.712\nEpoch 18 | Loss: 0.1780 | Val AUC: 0.701\nEpoch 19 | Loss: 0.1771 | Val AUC: 0.695\nEpoch 20 | Loss: 0.1733 | Val AUC: 0.710\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"Try hyperparameter tuning on the `hidden_dim`, learning rate, and number of `GINEConv` layers.","metadata":{}},{"cell_type":"code","source":"import optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:55:01.497088Z","iopub.execute_input":"2025-05-25T15:55:01.497410Z","iopub.status.idle":"2025-05-25T15:55:01.699070Z","shell.execute_reply.started":"2025-05-25T15:55:01.497387Z","shell.execute_reply":"2025-05-25T15:55:01.698568Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def objective(trial):\n    lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [192, 384, 768])\n    dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n    n_layers = trial.suggest_int(\"n_layers\", 3, 4)\n\n    test_model = GINEModel(\n        in_channels,\n        hidden_dim = hidden_dim,\n        num_tasks = num_tasks,\n        dropout = dropout,\n        n_layers = n_layers\n    ).to(device)\n\n    optimizer = torch.optim.Adam(\n        model.parameters(),\n        lr=lr,\n        weight_decay=weight_decay\n    )\n\n    for epoch in range(1, 11):\n        loss = train_epoch(test_model, train_loader, optimizer, criterion, device)\n        \n    ps_val, ys_val = evaluate(model, val_loader, device)\n    ys_val = ys_val.reshape(ps_val.shape)\n    aucs = []\n    for i in range(ps_val.shape[1]):\n        mask = ys_val[:, i] >= 0\n        if mask.sum() > 0:\n            aucs.append(roc_auc_score(ys_val[mask, i], ps_val[mask, i]))\n\n    return np.mean(aucs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:18:40.240082Z","iopub.execute_input":"2025-05-25T16:18:40.240815Z","iopub.status.idle":"2025-05-25T16:18:40.246906Z","shell.execute_reply.started":"2025-05-25T16:18:40.240791Z","shell.execute_reply":"2025-05-25T16:18:40.246110Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"study = optuna.create_study(\n    direction=\"maximize\",\n    sampler=optuna.samplers.TPESampler(),\n    pruner=optuna.pruners.MedianPruner()\n)\n\nstudy.optimize(objective, n_trials=20, timeout=3600)\n\nprint(\"Best AUC: \", study.best_value)\nprint(\"Best hyperparams: \", study.best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:18:40.985971Z","iopub.execute_input":"2025-05-25T16:18:40.986567Z","iopub.status.idle":"2025-05-25T16:25:06.662649Z","shell.execute_reply.started":"2025-05-25T16:18:40.986512Z","shell.execute_reply":"2025-05-25T16:25:06.661813Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"[I 2025-05-25 16:18:40,988] A new study created in memory with name: no-name-cae8272b-676b-4e90-887d-227eeb5071a6\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:19:00,947] Trial 0 finished with value: 0.7174598574712118 and parameters: {'lr': 0.00258709777543951, 'weight_decay': 0.000126524404497115, 'hidden_dim': 768, 'dropout': 0.17285110043010013, 'n_layers': 3}. Best is trial 0 with value: 0.7174598574712118.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:19:23,786] Trial 1 finished with value: 0.7163709779591758 and parameters: {'lr': 0.00044563857267675697, 'weight_decay': 1.0514300732872546e-06, 'hidden_dim': 768, 'dropout': 0.12717895439668586, 'n_layers': 4}. Best is trial 0 with value: 0.7174598574712118.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:19:42,150] Trial 2 finished with value: 0.7180745716883331 and parameters: {'lr': 5.90851202055132e-05, 'weight_decay': 0.0005012309628866446, 'hidden_dim': 192, 'dropout': 0.2005584563661218, 'n_layers': 3}. Best is trial 2 with value: 0.7180745716883331.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:20:04,893] Trial 3 finished with value: 0.7183850340737737 and parameters: {'lr': 0.0016499304050835036, 'weight_decay': 8.705842413611501e-06, 'hidden_dim': 768, 'dropout': 0.16768633378059078, 'n_layers': 4}. Best is trial 3 with value: 0.7183850340737737.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:20:25,911] Trial 4 finished with value: 0.7176987546940062 and parameters: {'lr': 0.0003361274533765945, 'weight_decay': 0.0005274702140559025, 'hidden_dim': 384, 'dropout': 0.17258092964928676, 'n_layers': 4}. Best is trial 3 with value: 0.7183850340737737.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:20:46,977] Trial 5 finished with value: 0.7186043013780884 and parameters: {'lr': 0.00013481209591174848, 'weight_decay': 0.00011618476623670669, 'hidden_dim': 384, 'dropout': 0.2681114813439789, 'n_layers': 4}. Best is trial 5 with value: 0.7186043013780884.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:21:05,488] Trial 6 finished with value: 0.7204225667225037 and parameters: {'lr': 0.0024961289728303456, 'weight_decay': 5.414234773393283e-06, 'hidden_dim': 384, 'dropout': 0.18105549093409565, 'n_layers': 3}. Best is trial 6 with value: 0.7204225667225037.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:21:25,126] Trial 7 finished with value: 0.7141685499966219 and parameters: {'lr': 0.0022653565309231826, 'weight_decay': 0.00021193188440927092, 'hidden_dim': 768, 'dropout': 0.2907467297089408, 'n_layers': 3}. Best is trial 6 with value: 0.7204225667225037.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:21:43,380] Trial 8 finished with value: 0.7153277937198927 and parameters: {'lr': 0.00021079463702590708, 'weight_decay': 1.617319032717843e-06, 'hidden_dim': 384, 'dropout': 0.18540576117581214, 'n_layers': 3}. Best is trial 6 with value: 0.7204225667225037.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:22:01,579] Trial 9 finished with value: 0.7192480729143264 and parameters: {'lr': 0.0001724469474796396, 'weight_decay': 1.4873871726583475e-06, 'hidden_dim': 192, 'dropout': 0.2558694670543099, 'n_layers': 3}. Best is trial 6 with value: 0.7204225667225037.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:22:20,004] Trial 10 finished with value: 0.7217452460955558 and parameters: {'lr': 0.0011203180868754237, 'weight_decay': 1.2964539908790686e-05, 'hidden_dim': 384, 'dropout': 0.10049300554274788, 'n_layers': 3}. Best is trial 10 with value: 0.7217452460955558.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:22:38,473] Trial 11 finished with value: 0.7149714177151277 and parameters: {'lr': 0.0010128400202672123, 'weight_decay': 1.3094611791100248e-05, 'hidden_dim': 384, 'dropout': 0.11244609030554298, 'n_layers': 3}. Best is trial 10 with value: 0.7217452460955558.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:22:56,608] Trial 12 finished with value: 0.7144135896733959 and parameters: {'lr': 0.004999286736993556, 'weight_decay': 6.166880899195077e-06, 'hidden_dim': 384, 'dropout': 0.22498283293765986, 'n_layers': 3}. Best is trial 10 with value: 0.7217452460955558.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:23:14,705] Trial 13 finished with value: 0.7240326163972816 and parameters: {'lr': 0.0008333090883296305, 'weight_decay': 3.4116326791858185e-05, 'hidden_dim': 384, 'dropout': 0.14275025219273693, 'n_layers': 3}. Best is trial 13 with value: 0.7240326163972816.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:23:32,865] Trial 14 finished with value: 0.720399076355572 and parameters: {'lr': 0.0009638682017841634, 'weight_decay': 4.026834021450531e-05, 'hidden_dim': 384, 'dropout': 0.14134953085825822, 'n_layers': 3}. Best is trial 13 with value: 0.7240326163972816.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:23:51,463] Trial 15 finished with value: 0.7203247220038019 and parameters: {'lr': 0.000711320379998767, 'weight_decay': 2.9598295280660204e-05, 'hidden_dim': 384, 'dropout': 0.10566598434307717, 'n_layers': 3}. Best is trial 13 with value: 0.7240326163972816.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:24:09,553] Trial 16 finished with value: 0.7155031474524823 and parameters: {'lr': 0.0013120629280928217, 'weight_decay': 5.1527817250186934e-05, 'hidden_dim': 192, 'dropout': 0.1478277968221187, 'n_layers': 3}. Best is trial 13 with value: 0.7240326163972816.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:24:30,370] Trial 17 finished with value: 0.7181262132646404 and parameters: {'lr': 0.0005946759753524152, 'weight_decay': 2.0931193263843405e-05, 'hidden_dim': 384, 'dropout': 0.13353974438045935, 'n_layers': 4}. Best is trial 13 with value: 0.7240326163972816.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:24:48,792] Trial 18 finished with value: 0.716548466001851 and parameters: {'lr': 0.00028184214781008435, 'weight_decay': 3.2712843005883784e-06, 'hidden_dim': 384, 'dropout': 0.10736115488232381, 'n_layers': 3}. Best is trial 13 with value: 0.7240326163972816.\n/tmp/ipykernel_35/3303365689.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n/tmp/ipykernel_35/3303365689.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n/tmp/ipykernel_35/3303365689.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.3)\n[I 2025-05-25 16:25:06,658] Trial 19 finished with value: 0.7184020283957134 and parameters: {'lr': 0.00010004922132426162, 'weight_decay': 6.084522581781914e-05, 'hidden_dim': 192, 'dropout': 0.153409328146241, 'n_layers': 3}. Best is trial 13 with value: 0.7240326163972816.\n","output_type":"stream"},{"name":"stdout","text":"Best AUC:  0.7240326163972816\nBest hyperparams:  {'lr': 0.0008333090883296305, 'weight_decay': 3.4116326791858185e-05, 'hidden_dim': 384, 'dropout': 0.14275025219273693, 'n_layers': 3}\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"### Evaluate Best Model","metadata":{}},{"cell_type":"code","source":"best_model = GINEModel(\n    in_channels,\n    hidden_dim = study.best_params[\"hidden_dim\"],\n    num_tasks = num_tasks,\n    dropout = study.best_params[\"dropout\"],\n    n_layers = study.best_params[\"n_layers\"]\n).to(device)\n\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=study.best_params[\"lr\"],\n    weight_decay=study.best_params[\"weight_decay\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:39:00.165924Z","iopub.execute_input":"2025-05-25T16:39:00.166705Z","iopub.status.idle":"2025-05-25T16:39:00.211168Z","shell.execute_reply.started":"2025-05-25T16:39:00.166681Z","shell.execute_reply":"2025-05-25T16:39:00.210374Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"print(best_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:39:05.268119Z","iopub.execute_input":"2025-05-25T16:39:05.268781Z","iopub.status.idle":"2025-05-25T16:39:05.272840Z","shell.execute_reply.started":"2025-05-25T16:39:05.268757Z","shell.execute_reply":"2025-05-25T16:39:05.272091Z"}},"outputs":[{"name":"stdout","text":"GINEModel(\n  (conv1): GINEConv(nn=Sequential(\n    (0): Linear(in_features=4, out_features=384, bias=True)\n    (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Linear(in_features=384, out_features=384, bias=True)\n  ))\n  (conv_block): ModuleList(\n    (0-2): 3 x GINEConv(nn=Sequential(\n      (0): Linear(in_features=384, out_features=384, bias=True)\n      (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): Linear(in_features=384, out_features=384, bias=True)\n    ))\n  )\n  (gate): GatedGraphConv(384, num_layers=3)\n  (bn_gate): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc): Linear(in_features=384, out_features=12, bias=True)\n  (dropout): Dropout(p=0.14275025219273693, inplace=False)\n)\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"for epoch in range(1, 21):\n    loss = train_epoch(best_model, train_loader, optimizer, criterion, device)\n    ps_val, ys_val = evaluate(best_model, val_loader, device)\n    ys_val = ys_val.reshape(ps_val.shape)\n    aucs = []\n    for i in range(ps_val.shape[1]):\n        mask = ys_val[:, i] >= 0\n        if mask.sum() > 0:\n            aucs.append(roc_auc_score(ys_val[mask, i], ps_val[mask, i]))\n    print(f\"Epoch {epoch:02d} | Loss: {loss:.4f} | Val AUC: {np.mean(aucs):.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:39:46.545825Z","iopub.execute_input":"2025-05-25T16:39:46.546424Z","iopub.status.idle":"2025-05-25T16:40:26.176054Z","shell.execute_reply.started":"2025-05-25T16:39:46.546402Z","shell.execute_reply":"2025-05-25T16:40:26.175260Z"}},"outputs":[{"name":"stdout","text":"Epoch 01 | Loss: 0.6977 | Val AUC: 0.516\nEpoch 02 | Loss: 0.6978 | Val AUC: 0.522\nEpoch 03 | Loss: 0.6981 | Val AUC: 0.515\nEpoch 04 | Loss: 0.6978 | Val AUC: 0.504\nEpoch 05 | Loss: 0.6972 | Val AUC: 0.512\nEpoch 06 | Loss: 0.6976 | Val AUC: 0.513\nEpoch 07 | Loss: 0.6983 | Val AUC: 0.529\nEpoch 08 | Loss: 0.6972 | Val AUC: 0.524\nEpoch 09 | Loss: 0.6968 | Val AUC: 0.523\nEpoch 10 | Loss: 0.6977 | Val AUC: 0.507\nEpoch 11 | Loss: 0.6982 | Val AUC: 0.523\nEpoch 12 | Loss: 0.6979 | Val AUC: 0.515\nEpoch 13 | Loss: 0.6972 | Val AUC: 0.538\nEpoch 14 | Loss: 0.6971 | Val AUC: 0.516\nEpoch 15 | Loss: 0.6971 | Val AUC: 0.509\nEpoch 16 | Loss: 0.6977 | Val AUC: 0.528\nEpoch 17 | Loss: 0.6971 | Val AUC: 0.524\nEpoch 18 | Loss: 0.6970 | Val AUC: 0.518\nEpoch 19 | Loss: 0.6971 | Val AUC: 0.540\nEpoch 20 | Loss: 0.6968 | Val AUC: 0.523\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"Somehow did worse...","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}